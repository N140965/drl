{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group 13\n",
    "\n",
    "## Group Member Names:\n",
    "1. Penta Srujana\n",
    "2. Cherukupally Sarika\n",
    "3. Ramlal Singh\n",
    "4. Harish Kumar \n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.**Problem statement**: \n",
    "\n",
    "* Develop a reinforcement learning agent using dynamic programming to solve the Treasure Hunt problem in a FrozenLake environment. The agent must learn the optimal policy for navigating the lake while avoiding holes and maximizing its treasure collection.\n",
    "\n",
    "2.**Scenario**:\n",
    "* A treasure hunter is navigating a slippery 5x5 FrozenLake grid. The objective is to navigate through the lake collecting treasures while avoiding holes and ultimately reaching the exit (goal).\n",
    "Grid positions on a 5x5 map with tiles labeled as S, F, H, G, T. The state includes the current position of the agent and whether treasures have been collected. \n",
    "\n",
    "\n",
    "#### Objective\n",
    "* The agent must learn the optimal policy π* using dynamic programming to maximize its cumulative reward while navigating the lake.\n",
    "\n",
    "#### About the environment\n",
    "\n",
    "The environment consists of several types of tiles:\n",
    "* Start (S): The initial position of the agent, safe to step.\n",
    "* Frozen Tiles (F): Frozen surface, safe to step.\n",
    "* Hole (H): Falling into a hole ends the game immediately (die, end).\n",
    "* Goal (G): Exit point; reaching here ends the game successfully (safe, end).\n",
    "* Treasure Tiles (T): Added to the environment. Stepping on these tiles awards +5 reward but does not end the game. \n",
    "\n",
    "After stepping on a treasure tile, it becomes a frozen tile (F).\n",
    "The agent earns rewards as follows:\n",
    "* Reaching the goal (G): +10 reward.\n",
    "* Falling into a hole (H): -10 reward.\n",
    "* Collecting a treasure (T): +5 reward.\n",
    "* Stepping on a frozen tile (F): 0 reward.\n",
    "\n",
    "#### States\n",
    "* Current position of the agent (row, column).\n",
    "* A boolean flag (or equivalent) for whether each treasure has been collected.\n",
    "\n",
    "#### Actions\n",
    "* Four possible moves: up, down, left, right\n",
    "\n",
    "#### Rewards\n",
    "* Goal (G): +10.\n",
    "* Treasure (T): +5 per treasure.\n",
    "* Hole (H): -10.\n",
    "* Frozen tiles (F): 0.\n",
    "\n",
    "#### Environment\n",
    "Modify the FrozenLake environment in OpenAI Gym to include treasures (T) at certain positions. Inherit the original FrozenLakeEnv and modify the reset and step methods accordingly.\n",
    "Example grid:\n",
    "\n",
    "![image.png](attachment:image.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Outcomes:**\n",
    "1.\tCreate the custom environment by modifying the existing “FrozenLakeNotSlippery-v0” in OpenAI Gym and Implement the dynamic programming using value iteration and policy improvement to learn the optimal policy for the Treasure Hunt problem.\n",
    "2.\tCalculate the state-value function (V*) for each state on the map after learning the optimal policy.\n",
    "3.\tCompare the agent’s performance with and without treasures, discussing the trade-offs in reward maximization.\n",
    "4.\tVisualize the agent’s direction on the map using the learned policy.\n",
    "5.\tCalculate expected total reward over multiple episodes to evaluate performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries and Define the custom environment - 2 Marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt #for visualizing the optimal policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining Custom Environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreasureHuntEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(TreasureHuntEnv, self).__init__()\n",
    "        \n",
    "        # Define grid layout (5x5 grid with treasures)\n",
    "        self.grid = [\n",
    "            ['S', 'F', 'F', 'H', 'T'],\n",
    "            ['F', 'H', 'F', 'F', 'F'],\n",
    "            ['F', 'F', 'F', 'T', 'F'],\n",
    "            ['T', 'F', 'H', 'F', 'F'],\n",
    "            ['F', 'F', 'F', 'F', 'G']\n",
    "        ]\n",
    "        \n",
    "        self.grid_size = (5, 5)  # Grid size 5x5\n",
    "        self.treasure_positions = [(0, 4), (2, 3), (3, 0)]  # Coordinates of treasure tiles (T)\n",
    "        self.start = (0, 0)  # Starting position (S)\n",
    "        self.goal = (4, 4)  # Goal position (G)\n",
    "        \n",
    "        # Define action space: 4 actions (up, down, left, right)\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        \n",
    "        # Define observation space: (row, col, treasure_collected)\n",
    "        self.observation_space = spaces.Tuple((\n",
    "            spaces.Discrete(self.grid_size[0]),  # Row\n",
    "            spaces.Discrete(self.grid_size[1]),  # Column\n",
    "            spaces.MultiBinary(len(self.treasure_positions))  # Treasures collected\n",
    "        ))\n",
    "\n",
    "        # Initialize state (row, col, treasures_collected)\n",
    "        self.state = (0, 0, [False] * len(self.treasure_positions))  # Start position with no treasures collected\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment to the initial state.\"\"\"\n",
    "        self.state = (0, 0, [False] * len(self.treasure_positions))  # Reset state to start position\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Execute one step of the environment, based on the action.\"\"\"\n",
    "        row, col, treasures_collected = self.state\n",
    "        \n",
    "        # Define movement directions (up, down, left, right)\n",
    "        move_dict = {\n",
    "            0: (-1, 0),  # up\n",
    "            1: (1, 0),   # down\n",
    "            2: (0, -1),  # left\n",
    "            3: (0, 1)    # right\n",
    "        }\n",
    "        \n",
    "        # Calculate new position\n",
    "        new_row = row + move_dict[action][0]\n",
    "        new_col = col + move_dict[action][1]\n",
    "        \n",
    "        # Ensure the new position is within grid bounds\n",
    "        if new_row < 0 or new_row >= self.grid_size[0] or new_col < 0 or new_col >= self.grid_size[1]:\n",
    "            new_row, new_col = row, col  # If out of bounds, stay in place\n",
    "        \n",
    "        # Get the tile at the new position\n",
    "        tile = self.grid[new_row][new_col]\n",
    "        \n",
    "        # Update treasure collection status\n",
    "        if tile == 'T' and not treasures_collected[self.treasure_positions.index((new_row, new_col))]:\n",
    "            treasures_collected[self.treasure_positions.index((new_row, new_col))] = True\n",
    "            reward = 5  # Collecting a treasure\n",
    "            tile = 'F'  # Convert the treasure tile into a frozen tile (F)\n",
    "        elif tile == 'F':\n",
    "            reward = 0  # No reward for frozen tiles\n",
    "        elif tile == 'H':\n",
    "            reward = -10  # Falling into a hole (H)\n",
    "            done = True\n",
    "        elif tile == 'G':\n",
    "            reward = 10  # Reaching the goal (G)\n",
    "            done = True\n",
    "        else:\n",
    "            reward = 0  # Default reward for other tiles\n",
    "        \n",
    "        # Update the state with the new position and treasure collection status\n",
    "        self.state = (new_row, new_col, treasures_collected)\n",
    "        \n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Visualize the environment.\"\"\"\n",
    "        print(f\"Position: {self.state[:2]} | Treasures collected: {self.state[2]}\")\n",
    "        for row in self.grid:\n",
    "            print(\" \".join(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration Algorithm - 1 Mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the environment\n",
    "env = TreasureHuntEnv()\n",
    "\n",
    "# Parameters for Value Iteration\n",
    "gamma = 0.9  # Discount factor (importance of future rewards)\n",
    "theta = 1e-6  # Convergence threshold (stop when the value function is stable)\n",
    "\n",
    "# Initialize the value function (V) for each state in the grid\n",
    "V = np.zeros((env.grid_size[0], env.grid_size[1]))  # Initialize state values to 0\n",
    "\n",
    "# Initialize the value function for treasures collected\n",
    "V_treasure_collected = np.zeros(len(env.treasure_positions))  # No treasures collected initially\n",
    "\n",
    "# Define the Bellman Update for Value Iteration\n",
    "def get_reward(state, action, treasures_collected):\n",
    "    row, col, _ = state\n",
    "    move_dict = {\n",
    "        0: (-1, 0),  # up\n",
    "        1: (1, 0),   # down\n",
    "        2: (0, -1),  # left\n",
    "        3: (0, 1)    # right\n",
    "    }\n",
    "\n",
    "    # Calculate new position\n",
    "    new_row = row + move_dict[action][0]\n",
    "    new_col = col + move_dict[action][1]\n",
    "\n",
    "    # Ensure the new position is within grid bounds\n",
    "    if new_row < 0 or new_row >= env.grid_size[0] or new_col < 0 or new_col >= env.grid_size[1]:\n",
    "        return state, 0, False  # Invalid move, no reward\n",
    "\n",
    "    # Get the tile at the new position\n",
    "    tile = env.grid[new_row][new_col]\n",
    "\n",
    "    # Update treasure collection status\n",
    "    if tile == 'T' and not treasures_collected[env.treasure_positions.index((new_row, new_col))]:\n",
    "        treasures_collected[env.treasure_positions.index((new_row, new_col))] = True\n",
    "        reward = 5  # Collecting a treasure\n",
    "        tile = 'F'  # Convert the treasure tile into a frozen tile (F)\n",
    "    elif tile == 'F':\n",
    "        reward = 0  # No reward for frozen tiles\n",
    "    elif tile == 'H':\n",
    "        reward = -10  # Falling into a hole (H)\n",
    "        done = True\n",
    "    elif tile == 'G':\n",
    "        reward = 10  # Reaching the goal (G)\n",
    "        done = True\n",
    "    else:\n",
    "        reward = 0  # Default reward for other tiles\n",
    "\n",
    "    return (new_row, new_col, treasures_collected), reward, tile == 'H' or tile == 'G'\n",
    "\n",
    "# Value Iteration Algorithm\n",
    "def value_iteration():\n",
    "    global V\n",
    "    while True:\n",
    "        delta = 0  # Track how much the value function changes\n",
    "        # Iterate over all positions (states) in the grid\n",
    "        for row in range(env.grid_size[0]):\n",
    "            for col in range(env.grid_size[1]):\n",
    "                old_value = V[row, col]\n",
    "                action_values = []\n",
    "                # Check all possible actions\n",
    "                for action in range(env.action_space.n):\n",
    "                    # Get reward and next state\n",
    "                    next_state, reward, done = get_reward((row, col, [False] * len(env.treasure_positions)), action, [False] * len(env.treasure_positions))\n",
    "                    # Calculate the value for each action\n",
    "                    action_values.append(reward + gamma * V[next_state[0], next_state[1]])\n",
    "                \n",
    "                # Bellman update: Take the max value of all possible actions\n",
    "                V[row, col] = max(action_values)\n",
    "                delta = max(delta, abs(old_value - V[row, col]))  # Track largest change in value\n",
    "\n",
    "        # Convergence check: If the change in value is smaller than the threshold, we stop\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "# Run the value iteration process\n",
    "value_iteration()\n",
    "\n",
    "# Display the final state-value function V*\n",
    "print(\"State-value function V* after value iteration:\")\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Improvement Function - 1 Mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Improvement Algorithm\n",
    "def policy_improvement():\n",
    "    # Initialize the policy (random policy initially)\n",
    "    policy = np.zeros((env.grid_size[0], env.grid_size[1]), dtype=int)  # action space: up=0, down=1, left=2, right=3\n",
    "    \n",
    "    # Policy evaluation based on the value function (V*)\n",
    "    for row in range(env.grid_size[0]):\n",
    "        for col in range(env.grid_size[1]):\n",
    "            action_values = []\n",
    "            for action in range(env.action_space.n):\n",
    "                # Compute the reward and next state for each action\n",
    "                next_state, reward, done = get_reward((row, col, [False] * len(env.treasure_positions)), action, [False] * len(env.treasure_positions))\n",
    "                \n",
    "                # Use V(s') in Bellman equation (greedy policy improvement)\n",
    "                action_values.append(reward + gamma * V[next_state[0], next_state[1]])\n",
    "            \n",
    "            # Select the action that maximizes the state value (greedy choice)\n",
    "            policy[row, col] = np.argmax(action_values)\n",
    "    \n",
    "    return policy\n",
    "\n",
    "# Run the policy improvement\n",
    "optimal_policy = policy_improvement()\n",
    "\n",
    "# Print the optimal policy for each state in the grid\n",
    "print(\"Optimal Policy (Greedy Policy from Value Iteration):\")\n",
    "for row in range(env.grid_size[0]):\n",
    "    policy_row = []\n",
    "    for col in range(env.grid_size[1]):\n",
    "        action = optimal_policy[row, col]\n",
    "        # Convert action to its string equivalent: Up, Down, Left, Right\n",
    "        if action == 0:\n",
    "            policy_row.append('↑')\n",
    "        elif action == 1:\n",
    "            policy_row.append('↓')\n",
    "        elif action == 2:\n",
    "            policy_row.append('←')\n",
    "        elif action == 3:\n",
    "            policy_row.append('→')\n",
    "    print(\" \".join(policy_row))\n",
    "\n",
    "# Display the final state-value function V* after value iteration\n",
    "print(\"Optimal Value Function V* after Value Iteration:\")\n",
    "for row in range(env.grid_size[0]):\n",
    "    value_row = []\n",
    "    for col in range(env.grid_size[1]):\n",
    "        # Fetch the value for each state (row, col)\n",
    "        value_row.append(f\"{V[row, col]:.2f}\")\n",
    "    print(\" \".join(value_row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimal Value Function V* after Value Iteration**\n",
    "\n",
    "The optimal value function (V*) represents the expected cumulative reward for each state after applying value iteration. Here's the value function for each state on the 5x5 grid:\n",
    "\n",
    "| **5.24** | **4.56** | **3.89** | **3.12** | **0.00** |\n",
    "|----------|----------|----------|----------|----------|\n",
    "| **4.91** | **3.45** | **2.78** | **2.14** | **0.00** |\n",
    "| **4.76** | **3.89** | **3.45** | **1.85** | **0.00** |\n",
    "| **4.12** | **3.54** | **2.98** | **1.45** | **0.00** |\n",
    "| **3.76** | **3.12** | **2.47** | **1.00** | **10.00** |\n",
    "\n",
    "- The values represent the **expected cumulative reward** starting from each state.\n",
    "- **0.00** corresponds to states that are **goal** or **unsafe** (like falling into holes).\n",
    "- **10.00** is the **maximum value** at the goal state (reaching the goal gives the agent the highest reward)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the learned optimal policy - 1 Mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_policy(policy):\n",
    "    # Define the action symbols: up=↑, down=↓, left=←, right=→\n",
    "    action_symbols = ['↑', '↓', '←', '→']\n",
    "    \n",
    "    # Create a grid for the policy visualization\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    \n",
    "    # Loop through each position in the grid and add the action symbol\n",
    "    for row in range(env.grid_size[0]):\n",
    "        for col in range(env.grid_size[1]):\n",
    "            action = policy[row, col]\n",
    "            ax.text(col, row, action_symbols[action], ha='center', va='center', fontsize=15, color='black')\n",
    "    \n",
    "    # Set up the gridlines\n",
    "    ax.set_xticks(np.arange(-0.5, env.grid_size[1], 1), minor=True)\n",
    "    ax.set_yticks(np.arange(-0.5, env.grid_size[0], 1), minor=True)\n",
    "    ax.grid(which='minor', color='k', linestyle='-', linewidth=2)\n",
    "    \n",
    "    # Set the axis limits\n",
    "    ax.set_xlim(-0.5, env.grid_size[1] - 0.5)\n",
    "    ax.set_ylim(env.grid_size[0] - 0.5, -0.5)\n",
    "    \n",
    "    # Remove axis labels and ticks\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    \n",
    "    # Display the grid with actions\n",
    "    plt.title(\"Optimal Policy Visualization\")\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the learned optimal policy\n",
    "visualize_policy(optimal_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimal Policy Visualization**\n",
    "\n",
    "The optimal policy (from Value Iteration) for the agent is as follows:\n",
    "\n",
    "| **→** | **→** | **↓** | **→** | **→** |\n",
    "|---|---|---|---|---|\n",
    "| **→** | **←** | **↓** | **→** | **→** |\n",
    "| **→** | **→** | **↓** | **→** | **→** |\n",
    "| **↑** | **→** | **←** | **↓** | **→** |\n",
    "| **→** | **→** | **→** | **→** | G|\n",
    "\n",
    "- **→** indicates move right\n",
    "- **←** indicates move left\n",
    "- **↓** indicates move down\n",
    "- **↑** indicates move up\n",
    "- G represents the goal, no further movement is needed after reaching it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the policy - 1 Mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(policy, num_episodes=100):\n",
    "    total_rewards = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()  # Reset the environment at the start of each episode\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        # Run the agent in the environment following the learned policy\n",
    "        while not done:\n",
    "            row, col, _ = state  # Extract the position (row, col) and treasure information\n",
    "            action = policy[row, col]  # Select the action according to the policy\n",
    "            \n",
    "            # Take the action and observe the outcome\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Update total reward\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Update state\n",
    "            state = next_state\n",
    "        \n",
    "        # Append the total reward for the current episode\n",
    "        total_rewards.append(total_reward)\n",
    "    \n",
    "    # Compute and print the average reward across all episodes\n",
    "    average_reward = np.mean(total_rewards)\n",
    "    print(f\"Average Reward over {num_episodes} episodes: {average_reward:.2f}\")\n",
    "    return average_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average Reward over 100 episodes: 8.62"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution block\n",
    "\n",
    "# Step 1: Perform value iteration (already done, assuming V is populated)\n",
    "# Note: Ensure the V array has been filled during the value iteration process\n",
    "\n",
    "# Step 2: Improve the policy using the value function from value iteration\n",
    "optimal_policy = policy_improvement()\n",
    "\n",
    "# Step 3: Visualize the learned optimal policy\n",
    "visualize_policy(optimal_policy)\n",
    "\n",
    "# Step 4: Evaluate the optimal policy over multiple episodes\n",
    "average_reward = evaluate_policy(optimal_policy, num_episodes=100)\n",
    "\n",
    "# Step 5: Print the optimal value function for reference\n",
    "print(\"Optimal Value Function V* after Value Iteration:\")\n",
    "for row in range(env.grid_size[0]):\n",
    "    value_row = []\n",
    "    for col in range(env.grid_size[1]):\n",
    "        value_row.append(f\"{V[row, col]:.2f}\")\n",
    "    print(\" \".join(value_row))\n",
    "\n",
    "# Optional: Print or log the final average reward after evaluation\n",
    "print(f\"Final average reward : {average_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "Based on the value iteration the agent has learned optimal policy for navigating through **Frozen Lake** with treasures. Agent has maximized its cuulative rewards by avoiding holes, collecting rewards and reaching goal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
