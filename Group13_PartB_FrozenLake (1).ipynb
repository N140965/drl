{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group 13\n",
    "\n",
    "## Group Member Names:\n",
    "1. Penta Srujana\n",
    "2. Cherukupally Sarika\n",
    "3. Ramlal Singh\n",
    "4. Harish Kumar \n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.**Problem statement**: \n",
    "\n",
    "* Develop a reinforcement learning agent using dynamic programming to solve the Treasure Hunt problem in a FrozenLake environment. The agent must learn the optimal policy for navigating the lake while avoiding holes and maximizing its treasure collection.\n",
    "\n",
    "2.**Scenario**:\n",
    "* A treasure hunter is navigating a slippery 5x5 FrozenLake grid. The objective is to navigate through the lake collecting treasures while avoiding holes and ultimately reaching the exit (goal).\n",
    "Grid positions on a 5x5 map with tiles labeled as S, F, H, G, T. The state includes the current position of the agent and whether treasures have been collected. \n",
    "\n",
    "\n",
    "#### Objective\n",
    "* The agent must learn the optimal policy π* using dynamic programming to maximize its cumulative reward while navigating the lake.\n",
    "\n",
    "#### About the environment\n",
    "\n",
    "The environment consists of several types of tiles:\n",
    "* Start (S): The initial position of the agent, safe to step.\n",
    "* Frozen Tiles (F): Frozen surface, safe to step.\n",
    "* Hole (H): Falling into a hole ends the game immediately (die, end).\n",
    "* Goal (G): Exit point; reaching here ends the game successfully (safe, end).\n",
    "* Treasure Tiles (T): Added to the environment. Stepping on these tiles awards +5 reward but does not end the game. \n",
    "\n",
    "After stepping on a treasure tile, it becomes a frozen tile (F).\n",
    "The agent earns rewards as follows:\n",
    "* Reaching the goal (G): +10 reward.\n",
    "* Falling into a hole (H): -10 reward.\n",
    "* Collecting a treasure (T): +5 reward.\n",
    "* Stepping on a frozen tile (F): 0 reward.\n",
    "\n",
    "#### States\n",
    "* Current position of the agent (row, column).\n",
    "* A boolean flag (or equivalent) for whether each treasure has been collected.\n",
    "\n",
    "#### Actions\n",
    "* Four possible moves: up, down, left, right\n",
    "\n",
    "#### Rewards\n",
    "* Goal (G): +10.\n",
    "* Treasure (T): +5 per treasure.\n",
    "* Hole (H): -10.\n",
    "* Frozen tiles (F): 0.\n",
    "\n",
    "#### Environment\n",
    "Modify the FrozenLake environment in OpenAI Gym to include treasures (T) at certain positions. Inherit the original FrozenLakeEnv and modify the reset and step methods accordingly.\n",
    "Example grid:\n",
    "\n",
    "![image.png](attachment:image.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Outcomes:**\n",
    "1.\tCreate the custom environment by modifying the existing “FrozenLakeNotSlippery-v0” in OpenAI Gym and Implement the dynamic programming using value iteration and policy improvement to learn the optimal policy for the Treasure Hunt problem.\n",
    "2.\tCalculate the state-value function (V*) for each state on the map after learning the optimal policy.\n",
    "3.\tCompare the agent’s performance with and without treasures, discussing the trade-offs in reward maximization.\n",
    "4.\tVisualize the agent’s direction on the map using the learned policy.\n",
    "5.\tCalculate expected total reward over multiple episodes to evaluate performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries and Define the custom environment - 2 Marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt #for visualizing the optimal policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining Custom Environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreasureHuntEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(TreasureHuntEnv, self).__init__()\n",
    "        \n",
    "        # Define grid layout (5x5 grid with treasures)\n",
    "        self.grid = [\n",
    "            ['S', 'F', 'F', 'H', 'T'],\n",
    "            ['F', 'H', 'F', 'F', 'F'],\n",
    "            ['F', 'F', 'F', 'T', 'F'],\n",
    "            ['T', 'F', 'H', 'F', 'F'],\n",
    "            ['F', 'F', 'F', 'F', 'G']\n",
    "        ]\n",
    "        \n",
    "        self.grid_size = (5, 5)  # Grid size 5x5\n",
    "        self.treasure_positions = [(0, 4), (2, 3), (3, 0)]  # Coordinates of treasure tiles (T)\n",
    "        self.start = (0, 0)  # Starting position (S)\n",
    "        self.goal = (4, 4)  # Goal position (G)\n",
    "        \n",
    "        # Define action space: 4 actions (up, down, left, right)\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        \n",
    "        # Define observation space: (row, col, treasure_collected)\n",
    "        self.observation_space = spaces.Tuple((\n",
    "            spaces.Discrete(self.grid_size[0]),  # Row\n",
    "            spaces.Discrete(self.grid_size[1]),  # Column\n",
    "            spaces.MultiBinary(len(self.treasure_positions))  # Treasures collected\n",
    "        ))\n",
    "\n",
    "        # Initialize state (row, col, treasures_collected)\n",
    "        self.state = (0, 0, [False] * len(self.treasure_positions))  # Start position with no treasures collected\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment to the initial state.\"\"\"\n",
    "        self.state = (0, 0, [False] * len(self.treasure_positions))  # Reset state to start position\n",
    "        return self.state\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Execute one step of the environment, based on the action.\"\"\"\n",
    "        \n",
    "        row, col, treasures_collected = self.state\n",
    "\n",
    "        # Define movement directions (up, down, left, right)\n",
    "        move_dict = {\n",
    "            0: (-1, 0),  # up\n",
    "            1: (1, 0),   # down\n",
    "            2: (0, -1),  # left\n",
    "            3: (0, 1)    # right\n",
    "        }\n",
    "\n",
    "        # Calculate new position\n",
    "        new_row = row + move_dict[action][0]\n",
    "        new_col = col + move_dict[action][1]\n",
    "\n",
    "        # Ensure the new position is within grid bounds\n",
    "        if new_row < 0 or new_row >= self.grid_size[0] or new_col < 0 or new_col >= self.grid_size[1]:\n",
    "            new_row, new_col = row, col  # If out of bounds, stay in place\n",
    "\n",
    "        # Get the tile at the new position\n",
    "        tile = self.grid[new_row][new_col]\n",
    "\n",
    "        # Initialize `done` to False\n",
    "        done = False\n",
    "\n",
    "        # Update treasure collection status\n",
    "        if tile == 'T' and not treasures_collected[self.treasure_positions.index((new_row, new_col))]:\n",
    "            treasures_collected[self.treasure_positions.index((new_row, new_col))] = True\n",
    "            reward = 5  # Collecting a treasure\n",
    "            tile = 'F'  # Convert the treasure tile into a frozen tile (F)\n",
    "        elif tile == 'F':\n",
    "            reward = 0  # No reward for frozen tiles\n",
    "        elif tile == 'H':\n",
    "            reward = -10  # Falling into a hole (H)\n",
    "            done = True\n",
    "        elif tile == 'G':\n",
    "            reward = 10  # Reaching the goal (G)\n",
    "            done = True\n",
    "        else:\n",
    "            reward = 0  # Default reward for other tiles\n",
    "\n",
    "        # Update the state with the new position and treasure collection status\n",
    "        self.state = (new_row, new_col, treasures_collected)\n",
    "\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Visualize the environment.\"\"\"\n",
    "        print(f\"Position: {self.state[:2]} | Treasures collected: {self.state[2]}\")\n",
    "        for row in self.grid:\n",
    "            print(\" \".join(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration Algorithm - 1 Mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State-value function V* after value iteration:\n",
      "[[29.2235168  31.61557512 35.12841761 39.53157585 38.36841826]\n",
      " [32.47057512 35.12841761 39.03157585 43.36841826 42.63157644]\n",
      " [36.07841761 39.03157585 43.36841826 42.63157644 47.36841879]\n",
      " [34.53157585 38.36841826 42.63157644 47.36841879 52.63157691]\n",
      " [38.36841826 42.63157644 47.36841879 52.63157691 47.36841922]]\n"
     ]
    }
   ],
   "source": [
    "# Set up the environment\n",
    "env = TreasureHuntEnv()\n",
    "\n",
    "# Parameters for Value Iteration\n",
    "gamma = 0.9  # Discount factor (importance of future rewards)\n",
    "theta = 1e-6  # Convergence threshold (stop when the value function is stable)\n",
    "\n",
    "# Initialize the value function (V) for each state in the grid\n",
    "V = np.zeros((env.grid_size[0], env.grid_size[1]))  # Initialize state values to 0\n",
    "\n",
    "# Initialize the value function for treasures collected\n",
    "V_treasure_collected = np.zeros(len(env.treasure_positions))  # No treasures collected initially\n",
    "\n",
    "# Define the Bellman Update for Value Iteration\n",
    "def get_reward(state, action, treasures_collected):\n",
    "    row, col, _ = state\n",
    "    move_dict = {\n",
    "        0: (-1, 0),  # up\n",
    "        1: (1, 0),   # down\n",
    "        2: (0, -1),  # left\n",
    "        3: (0, 1)    # right\n",
    "    }\n",
    "\n",
    "    # Calculate new position\n",
    "    new_row = row + move_dict[action][0]\n",
    "    new_col = col + move_dict[action][1]\n",
    "\n",
    "    # Ensure the new position is within grid bounds\n",
    "    if new_row < 0 or new_row >= env.grid_size[0] or new_col < 0 or new_col >= env.grid_size[1]:\n",
    "        return state, 0, False  # Invalid move, no reward\n",
    "\n",
    "    # Get the tile at the new position\n",
    "    tile = env.grid[new_row][new_col]\n",
    "\n",
    "    # Initialize done variable\n",
    "    done = False\n",
    "\n",
    "    # Update treasure collection status\n",
    "    if tile == 'T' and not treasures_collected[env.treasure_positions.index((new_row, new_col))]:\n",
    "        treasures_collected[env.treasure_positions.index((new_row, new_col))] = True\n",
    "        reward = 5  # Collecting a treasure\n",
    "        tile = 'F'  # Convert the treasure tile into a frozen tile (F)\n",
    "    elif tile == 'F':\n",
    "        reward = 0  # No reward for frozen tiles\n",
    "    elif tile == 'H':\n",
    "        reward = -10  # Falling into a hole (H)\n",
    "        done = True\n",
    "    elif tile == 'G':\n",
    "        reward = 10  # Reaching the goal (G)\n",
    "        done = True\n",
    "    else:\n",
    "        reward = 0  # Default reward for other tiles\n",
    "\n",
    "    return (new_row, new_col, treasures_collected), reward, done\n",
    "\n",
    "\n",
    "# Value Iteration Algorithm\n",
    "def value_iteration():\n",
    "    global V\n",
    "    while True:\n",
    "        delta = 0  # Track how much the value function changes\n",
    "        # Iterate over all positions (states) in the grid\n",
    "        for row in range(env.grid_size[0]):\n",
    "            for col in range(env.grid_size[1]):\n",
    "                old_value = V[row, col]\n",
    "                action_values = []\n",
    "                # Check all possible actions\n",
    "                for action in range(env.action_space.n):\n",
    "                    # Get reward and next state\n",
    "                    next_state, reward, done = get_reward((row, col, [False] * len(env.treasure_positions)), action, [False] * len(env.treasure_positions))\n",
    "                    # Calculate the value for each action\n",
    "                    action_values.append(reward + gamma * V[next_state[0], next_state[1]])\n",
    "                \n",
    "                # Bellman update: Take the max value of all possible actions\n",
    "                V[row, col] = max(action_values)\n",
    "                delta = max(delta, abs(old_value - V[row, col]))  # Track largest change in value\n",
    "\n",
    "        # Convergence check: If the change in value is smaller than the threshold, we stop\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "# Run the value iteration process\n",
    "value_iteration()\n",
    "\n",
    "# Display the final state-value function V*\n",
    "print(\"State-value function V* after value iteration:\")\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Improvement Function - 1 Mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy (Greedy Policy from Value Iteration):\n",
      "↓ → ↓ → ↓\n",
      "↓ ↓ ↓ ↓ ↓\n",
      "↓ → → ↓ ↓\n",
      "↓ ↓ ↓ ↓ ↓\n",
      "→ → → → ↑\n",
      "Optimal Value Function V* after Value Iteration:\n",
      "29.22 31.62 35.13 39.53 38.37\n",
      "32.47 35.13 39.03 43.37 42.63\n",
      "36.08 39.03 43.37 42.63 47.37\n",
      "34.53 38.37 42.63 47.37 52.63\n",
      "38.37 42.63 47.37 52.63 47.37\n"
     ]
    }
   ],
   "source": [
    "# Policy Improvement Algorithm\n",
    "def policy_improvement():\n",
    "    # Initialize the policy (random policy initially)\n",
    "    policy = np.zeros((env.grid_size[0], env.grid_size[1]), dtype=int)  # action space: up=0, down=1, left=2, right=3\n",
    "    \n",
    "    # Policy evaluation based on the value function (V*)\n",
    "    for row in range(env.grid_size[0]):\n",
    "        for col in range(env.grid_size[1]):\n",
    "            action_values = []\n",
    "            for action in range(env.action_space.n):\n",
    "                # Compute the reward and next state for each action\n",
    "                next_state, reward, done = get_reward((row, col, [False] * len(env.treasure_positions)), action, [False] * len(env.treasure_positions))\n",
    "                \n",
    "                # Use V(s') in Bellman equation (greedy policy improvement)\n",
    "                action_values.append(reward + gamma * V[next_state[0], next_state[1]])\n",
    "            \n",
    "            # Select the action that maximizes the state value (greedy choice)\n",
    "            policy[row, col] = np.argmax(action_values)\n",
    "    \n",
    "    return policy\n",
    "\n",
    "# Run the policy improvement\n",
    "optimal_policy = policy_improvement()\n",
    "\n",
    "# Print the optimal policy for each state in the grid\n",
    "print(\"Optimal Policy (Greedy Policy from Value Iteration):\")\n",
    "for row in range(env.grid_size[0]):\n",
    "    policy_row = []\n",
    "    for col in range(env.grid_size[1]):\n",
    "        action = optimal_policy[row, col]\n",
    "        # Convert action to its string equivalent: Up, Down, Left, Right\n",
    "        if action == 0:\n",
    "            policy_row.append('↑')\n",
    "        elif action == 1:\n",
    "            policy_row.append('↓')\n",
    "        elif action == 2:\n",
    "            policy_row.append('←')\n",
    "        elif action == 3:\n",
    "            policy_row.append('→')\n",
    "    print(\" \".join(policy_row))\n",
    "\n",
    "# Display the final state-value function V* after value iteration\n",
    "print(\"Optimal Value Function V* after Value Iteration:\")\n",
    "for row in range(env.grid_size[0]):\n",
    "    value_row = []\n",
    "    for col in range(env.grid_size[1]):\n",
    "        # Fetch the value for each state (row, col)\n",
    "        value_row.append(f\"{V[row, col]:.2f}\")\n",
    "    print(\" \".join(value_row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the learned optimal policy - 1 Mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZwAAAGwCAYAAACdL9N0AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAIXtJREFUeJzt3QuUlHX9x/HvLHtBkB08mu4uLMiWdjlpmWhYqSUtRMkKuQpGSWpECJSc6BBpWZ5qS6MQWJC0I15KgyQWuhy7SSYIgUZqCngBzwKrEC6zxrK7sPv8z/d3/rNn7zvPw+53Zp55v84Zx53rb77PM/N5fpcZIp7neQIAQD/L6u8nAABAETgAABMEDgDABIEDADBB4AAATBA4AAATBA4AwASBAwAwQeAAAEwQOBlo1apVEolEZO/evRnz3GeffbZ88YtfbP1748aNrh16noqSuY16a8fHP/5xd7KWrOdF3yFwUsB//vMf+fznPy/Dhg2TvLw8KSoqkmnTprnLT8YPf/hDWbdunaSj7373u+6DLn4aNGiQvO9975PbbrtN6urqJN2UlZW51/D22293exvd5rm5uXL48GHJVC+++KLb9skOWvQT/S01JM9jjz3m5ebmegUFBd6tt97q3Xfffd5tt93mFRYWusvXrl0b+LEHDx7sTZ8+vdPlJ06c8I4dO+a1tLR41u6//3797T5vz549Pd7u9ttvd7dbsWKF99BDD7nzyZMnu8suueQS320fOXJku1o0Nze7Gui5hUcffdS1/YEHHujy+qNHj7rtNXHixKRvo962V2Njozv1hzVr1rjne+KJJzpd15/PCxvZ/RVk6N2rr74qX/jCF6SkpESefPJJecc73tF63de+9jW59NJL3fXPPfecu01fGTBggDulg/LycjnjjDPc/3/lK1+Rq6++WtauXStbtmyRSy65JPDjZmVlycCBA8WyhzNkyBD51a9+Jddff32n66uqquTo0aOul5Pq20h7YZn0vOg7DKkl0V133SX19fXy85//vF3YKP2QXblypfsQuvPOOzsNNe3cuVOuvfZayc/Pl9NPP90FVENDQ+vt9DZ63wceeKB1WCo+h9HVuLzOcVx55ZVuTmP06NFyyimnyHnnndc6x6Ef8vq3fkhfeOGF8q9//atdezUU9fE1GPU2BQUFcuONN/b58NAVV1zhzvfs2ePO9TV+/etfl+LiYjcc+e53v1t+8pOfaM+9x8fpbg5n69at8ulPf1pOO+00GTx4sJx//vly9913u+vuv/9+d5+Orz0+fKkBsX///i6fT+v52c9+Vv7617/KwYMHO12vQaSBpMHU3Tbavn27jB8/3u0b+nijRo1yNe7tNelj6OX6mH2xvTrOpei+03b4s+0p3pbXX39dbr75Zrd9tO26z15zzTXtXp+2Ty9Tn/jEJzo9RldzOFrLm266Sc466yz3Oj7wgQ+4fb6r16/7hb7X3vnOd7p95aKLLpJt27b1+nrRd+jhJNGGDRvcm1V7Ml257LLL3PW///3vO12nYaPXVVRUuKP9JUuWSG1trTz44IPu+oceeki+9KUvycUXXyxf/vKX3WX6RuvJK6+8Ip/73Odk5syZbk5J36ATJ06Ue+65R771rW+5Dwylz6nPv2vXLtdTUH/+85/ltddekxtuuMF9eOn8k7659Vzbp2/4vuoVKv3A0lDRD+gnnnjCfeh88IMflMcff1y+8Y1vuA/+n/3sZ74eW1+Dhm5hYaELcH0dL730kvzud79zf2tva/bs2fLLX/5SLrjggnb31cv0w1Dn4bqjvRf9MFy9erXMmTOn9fK33nrLtfu6665zH8Zd0Q/WcePGuQOTb37zmzJ06FD3QaoHAkH05fZavHix/O9//2t3mdZ+x44dbjsp/WDfvHmzTJ06VYYPH+7avmLFClcznbfR+S3d37/61a+6fVn3t/e+973uvvHzjo4dO+bur/ut1lMDeM2aNS5Ijxw54rZZx1DXOTTdv/X16YGcHgRoHXJycgJUEb4ZDd2hgyNHjrix6quuuqrH25WVlbnb1dXVtZvb0Mvbuvnmm93l//73v3udw+lqXF7nOPSyzZs3t172+OOPu8tOOeUU7/XXX2+9fOXKlZ3G2evr6zs9zyOPPOJu9+STT/b43F2Jv85du3Z5hw4dcrfX583Ly/POOussN+exbt06d5vvf//77e5bXl7uRSIR75VXXmn3+trWQtve9jXonMmoUaPc7Wpra9s9Xtt5lOuuu84rKipqN/fz7LPPusfS19YTfQ6dm9M5qLbuueced3+td3d1+u1vf+v+3rZtW7eP3/E1xeljdGzfyWyvyy+/3J26s3r1anefO+64o8fne/rpp93tHnzwwYTmcDo+7+LFi91tH3744dbLmpqaXH1PPfXU1vdM/PWffvrp3ltvvdV626qqKnf5hg0bun0t6FsMqSVJfLWSDqP0JH59x5VZeqTd1ty5c935H/7wh8Bt0lVgbedFPvzhD7cOY40YMaLT5XpkGNf2yFyH9v773//KmDFj3N/PPvts4DbpEIwe1evRqx6Zvutd73I9Pj0i1teqw1h6VNyWDrFp7+ePf/xjws+jw2Q6THfLLbe43kNbbY/2df7lwIEDrlfVtnejr1/nl3qibdUj/KeffrrdUJIeeeuQ0NixY7u9b7xN2ts6fvy4nKz+2l7aW9GhuauuusqtKOzq+bT9OnSn21JfV9Dn0+2vvTPtGcZpT0X3B+1x/f3vf293+ylTprih0rj4yELb/Rj9i8BJkniQ9LRMtqdgOuecc9r9rcNlOrx1MstJ24aKikaj7lznR7q6XIfw2g4L6RCGfnDqh0s8JFQsFgvcpscee8wN/+g4vg6dvPDCC24OKT4voEvIO9YmPgSj1/sdqnv/+9/f4+1KS0vdkJuGjGppaZFHHnnEfcD2dvCg4osCNGTUvn375B//+IcLop4WCVx++eUu0L73ve+5ORx9Pp1TamxslCD6Y3vpQZEOUemwog7ttg1qHf76zne+0zrXpq9Bn1OHvoI+n25ffR/Eh3V72/4d9+94+LTdj9G/mMNJEv3Q1g8unbztiV6vb2BdHNCTvpgj6e4Dr7vL207M65yOjtHr/InOpZx66qnuw/hTn/qUOw9Kx/Xjq9RSgdZC57nuvfdeWb58uWzatMn1eHTOKxEalu95z3tcSOk8hZ5rHeNB1NP2/c1vfuPmV3TuT+d8tCexaNEid5nWu7t9oLm5udNl/bG9dO5Ea/HPf/6z0/6qPXANSO1Bai9a939trwbtyewffiSyH6N/0cNJIp2g1mGcp556qsvr9chXeyx6u45efvnldn/r0b++cXUhQVxfTdT3Ro8QdfWVTmbrEfjkyZNdT6Avl3J3ZeTIke4DrmMvUVfwxa9PVHxBhfageqPDano0rx/82tPRI3VdPZYoDRd9Hj2Y0J6OHqXriqlE6LDXD37wA7diTZ9bJ/kfffTRdkfs2mtoq+ORfn9srx/96EfuS8bas9FA7UjDcvr06S4gdfGFPt/HPvaxTm31s8/q9tX3QcfACrL9YYPASSI9utThDJ2b6LgcVYc89HsnOleht+uosrKy3d9Lly515xMmTGi9TJf1dnxD9+eRY8cjRV291J90+bIevS9btqzTCin94Gpbi9586EMfckNK2uaONev4unSptJ7uu+8+N+SnR+nZ2YkPFsR7MzrEpCu5euvdxEOiYzu0Z6Liw2r6AavbQr/T1Zb2xPpze/3lL39x8zW33nqrTJo0qcvb6HN2fD7dZzv2vnSfVYnst7r933jjDfn1r3/detmJEyfc42qPTYchkVoYUksiPbLVZbL6gaPfcdGlvfqhp72aX/ziF24iV4dculrOrD0jXRKsQyA6Cf3www+7oR79HkLb4Rv9MPjpT3/q5jr0seMT/n1Jh0906EuXmeqEsA4B/ulPf2r9rkx/0SXb+n0N/aDTmulr1+fVL1Hq0E1vy8Db0nkAXaarj6kf5LpcWIc89WhZexE6hNWxlzN//nz3/4kOp8XpdvjIRz7i2qkSCRzdTzQ4tDeir0t7dTqsp7XXD16lw1T6PRb9wNXA1dvpIoOO3/vp6+2lk/bay9P9WffDtrQno/NE2kvXpfraRl2covus7pvxZdNxWnsNpx//+Mdubkfne3TRyplnntnpeXW5v35XTYfynnnmGde7156UDnNqeCYypwZjfbzqDQE899xzbrmtLpnNyclxP3Ojfz///PPdLhd+8cUX3fLfIUOGeKeddpo3Z84c91Mobe3cudO77LLL3LJmvU98WXB3y6I/85nPdHo+vd3s2bPbXRZfZnrXXXe1XrZv3z730zNDhw71otGod80113gHDhxwt9M2B10WrUuie/L222978+bNc0uVtXbnnHOOa1fHn4TpbVl03FNPPeWVlpa6uuqy8vPPP99bunRpp+etqanxBgwY4J177rleEJWVle75L7744i6v71gnXXqt+8SIESPc0vAzzzzTu/LKK73t27e3u5/W6+qrr/YGDRrk9ouZM2d6L7zwQqdl0SezvTouT9bruzvF66tLzW+44QbvjDPOcEuWx48f7/bPjttF3XvvvV5JSYmrb9vH6Go59ptvvtn6uPpTUOedd16n5eld7a9t29729aJ/RfQ/1iGH4PSXBnTc/dChQyk1mZ5ptPepPSAdFvv2t7+d7OYAaYE5HCAA/RkWnX/Q37oDkBjmcAAf/va3v7kvN+pKMZ0gb7sqEEDPCBzAhzvuuMN9f+WjH/1o68pAAIlhDgcAYII5HACACQIHAJDaczj6cxL6syL65Sqrn1ABAKQWnZXRLyLrl8s7/pDqSQeO/qSKnpqamlp/YRcAkNmqq6vdP67XL4sG9Gcn4v9Gh34BDr3T333ScmuPUP8dDySGuvlHzYKhbv7V1NS0/v5d/J8u6fMhtfgwmoaNDq2hd5r++k8fa9dT/x0UJIa6+UfNgqFu/mmtNHQSmVph0QAAwASBAwAwQeAAAEwQOAAAEwQOAMAEgQMAMEHgAABMEDgAABMEDgDABIEDADBB4AAATBA4AAATBA4AwASBAwAwQeAAAEwQOAAAEwQOAMAEgQMAMEHgAABMEDgAABMEDgDABIEDADBB4AAATBA4AAATBA4AwASBAwAwEbrAqa+vl3Xr1smOHTuS3RSEHPsarNSHZF8LXeAcPHhQJk+eLMuWLUt2UxBy7GuwcjAk+1roAgcAkJoIHHRr69atyW4CMgT7WmYgcNCl5cuXy5gxY2TRokXJbgpCjn0tcxA46NKECRNkxIgRMn/+fFm8eHGym4MQY1/LHAQOujRq1CjZuHGj+yCYN29e2k9WInWxr2WO7GQ3ALamTp3qa2llQ0ODO587d64UFBRIeXl5P7YOYcK+ho4InAyzd+9e2bVrV6D7Hj58uM/bg/BiX0PgIbXGxkapq6trd0L62bJli3iel9CptrZWRo8e3XrUOXPmzGQ3H2mEfQ2BA6eiokKi0Wjrqbi4WFLtm7hdaWpqkubmZvP2pLtYLCbjxo2T7du3y6xZs2TJkiXJblLKYF/rW+xrmbOvJRw4CxcudDtG/FRdXS2pQiccS0pKZNOmTe0uP378uBsHnjZtWlpunGQvVd22bZvMmDFDKisrk92clMG+1vfY1zJnX0s4cPLy8iQ/P7/dKVUcPXrUdcl1eeXmzZvdZSdOnJApU6bIhg0b3PBfum2YZFuwYIGsWrVKVq5cKZFIJNnNSRnsa32PfS2D9jUvoFgs5undCwsLvVSwfv16Lzc318vJyXHtip+PHz/ea2ho8FLBsGHDXJv0HOlbN/a18Eq1uq1Pg31NM0DbpJnQm9B8D2fixImyZs2adt3O0tJS9wur2jsD+gr7GqxMDNm+FprAUWVlZbJ69WrJycmRsWPHSlVVlQwcODDZzUIIsa/BSlmI9rXQfQ9n0qRJcujQIRk8eLBkZ4fu5SGFsK/ByqSQ7Gvp2/Ie6LJtwAL7GqxEQ7CvhWpIDQCQuggcAIAJAgcAYILAAQCYIHAAACYIHACACQIHAGCCwAEAmCBwAAAmCBwAgAkCBwBggsABAJggcAAAJggcAIAJAgcAYILAAQCYIHAAACYIHACACQIHAGCCwAEAmCBwAAAmCBwAgAkCBwBggsABAJggcAAAJggcAIAJAgcAYILAAQCYIHAAACYinud5Qe5YV1cn0WhUIpGIFBUV9X3LQqimpkZaWlokKytLCgsLk92ctEHd/KNmwVA3/w4cOCAaI7FYTPLz8/s2cCorK92publZdu/eHaB5AICw6ZfAiaOH4x9HT8FQN/+oWTDUrX97ONlykgoKCmTfvn0n+zAZYfjw4bJ//363I1OzxFE3/6hZMNTNP+1waFAngkUDAAATBA4AwASBAwAwQeAAAEwQOAAAEwQOAMAEgQMAMEHgAABMEDgAABMEDgDABIEDADBB4AAATBA4AAATBA4AwASBAwAwQeAAAEwQOAAAEwQOAMAEgQMAMEHgAABMEDgAABMEDgDABIEDADBB4AAATBA4AAATBA4AwEToAqe+vl7WrVsnO3bsSHZT0gp184+aBUPdMrdmoQucgwcPyuTJk2XZsmXJbkpaoW7+UbNgqFvm1ix0gQMASE0EDgDABIEDADBB4AAATBA4AAATBA4AwASBAwAwkZ3oDRsbG90prq6urr/aBADI5B5ORUWFRKPR1lNxcbGk2jdxu9LU1CTNzc3m7UkX1M0/ahYMdfMvbDVLOHAWLlwosVis9VRdXS2pYuPGjVJSUiKbNm1qd/nx48elvLxcpk2blpYbp79RN/+oWTDUzb8w1izhwMnLy5P8/Px2p1Rx9OhRqa2tlQkTJsjmzZvdZSdOnJApU6bIhg0b3PBfum0YC9TNP2oWDHXzL5Q18wKKxWKe3r2wsNBLBevXr/dyc3O9nJwc1674+fjx472GhgYvFQwbNsy1Sc9TBXXzj5oFQ93CWTPNAG2TZkJvQhM4qqqqqnWD6Km0tNQ7duyYlypSbWeOo27+UbNgqFv4auYncEK1LLqsrExWr14tOTk5MnbsWKmqqpKBAwcmu1kpj7r5R82CoW6ZXbOEl0Wni0mTJsmhQ4dk8ODBkp0dupfXb6ibf9QsGOqWuTVL35b3QJdtwz/q5h81C4a6ZWbNQjWkBgBIXQQOAMAEgQMAMEHgAABMEDgAABMEDgDABIEDADBB4AAATBA4AAATBA4AwASBAwAwQeAAAEwQOAAAEwQOAMAEgQMAMEHgAABMEDgAABMEDgDABIEDADBB4AAATBA4AAATBA4AwASBAwAwQeAAAEwQOAAAEwQOAMAEgQMAMEHgAABMEDgAABMRz/O8IHesq6uTaDQqkUhEioqK+r5lIVRTUyMtLS2SlZUlhYWFyW5O2qBu/lGzYKibfwcOHBCNkVgsJvn5+X0bOJWVle7U3Nwsu3fvDtA8AEDY9EvgxNHD8Y+jp2Com3/ULBjq1r89nGw5SQUFBbJv376TfZiMMHz4cNm/f7/bkalZ4qibf9QsGOrmn3Y4NKgTwaIBAIAJAgcAYILAAQCYIHAAACYIHACACQIHAGCCwAEAmCBwAAAmCBwAgAkCBwBggsABAJggcAAAJggcAIAJAgcAYILAAQCYIHAAACYIHACACQIHAGCCwAEAmCBwAAAmCBwAgAkCBwBggsABAJggcAAAJggcAIAJAgcAYCJ0gVNfXy/r1q2THTt2JLspALrAezRzaxa6wDl48KBMnjxZli1bluymAOgC79HMrVnoAgcAkJoIHHRr69atyW5C2qFmQPcIHHRp+fLlMmbMGFm0aFGym5I2qBnQMwIHXZowYYKMGDFC5s+fL4sXL052c9ICNQN6RuCgS6NGjZKNGze6D9B58+al/WSlBWoG9Cy7l+sRMlOnTvW1tLKhocGdz507VwoKCqS8vFwyDTUD+gaBk2H27t0ru3btCnTfw4cPSyaiZoDxkFpjY6PU1dW1OyH9bNmyRTzPS+hUW1sro0ePbj1anzlzpmQiagYYB05FRYVEo9HWU3FxsaTaN3G70tTUJM3NzebtSXexWEzGjRsn27dvl1mzZsmSJUuS3aSUR816xnvUv7DVLOHAWbhwoXtDxU/V1dWSKnSitqSkRDZt2tTu8uPHj7vx82nTpqXlxkn2Et9t27bJjBkzpLKyMtnNSQvUrHu8R/0LY80SDpy8vDzJz89vd0oVR48edUMZuix18+bN7rITJ07IlClTZMOGDW74L902TLItWLBAVq1aJStXrpRIJJLs5qQFatY93qP+hbJmXkCxWMzTuxcWFnqpYP369V5ubq6Xk5Pj2hU/Hz9+vNfQ0OClgmHDhrk26TkSR93CUTPeo+GsmWaAtkkzoTeh+R7OxIkTZc2aNe26naWlpe4XVrV3BiC5eI/6F7aahSZwVFlZmaxevVpycnJk7NixUlVVJQMHDkx2swD8P96jmV2z0H0PZ9KkSXLo0CEZPHiwZGeH7uUBaY/3aObWLH1b3gNdtg0gdfEezcyahWpIDQCQuggcAIAJAgcAYILAAQCYIHAAACYIHACACQIHAGCCwAEAmCBwAAAmCBwAgAkCBwBggsABAJggcAAAJggcAIAJAgcAYILAAQCYIHAAACYIHACACQIHAGCCwAEAmCBwAAAmCBwAgAkCBwBggsABAJggcAAAJggcAIAJAgcAYILAAQCYIHAAACYinud5Qe5YV1cn0WhUIpGIFBUV9X3LQqimpkZaWlokKytLCgsLk92ctEHd/KNmwVA3/w4cOCAaI7FYTPLz8/s2cCorK92publZdu/eHaB5AICw6ZfAiaOH4x9HT8FQN/+oWTDUrX97ONlykgoKCmTfvn0n+zAZYfjw4bJ//363I1OzxFE3/6hZMNTNP+1waFAngkUDAAATBA4AwASBAwAwQeAAAEwQOAAAEwQOAMAEgQMAMEHgAABMEDgAABMEDgDABIEDADBB4AAATBA4AAATBA4AwASBAwAwQeAAAEwQOAAAEwQOAMAEgQMAMEHgAABMEDgAABMEDgDABIEDADBB4AAATBA4AAATBA4AwEToAqe+vl7WrVsnO3bsSHZT0gp184+aBUPdMrdmoQucgwcPyuTJk2XZsmXJbkpaoW7+UbNgqFvm1ix0gQMASE0EDgDABIEDADBB4AAATBA4AAATBA4AwASBAwAwkZ3oDRsbG90prq6urr/aBADI5B5ORUWFRKPR1lNxcbGk2jdxu9LU1CTNzc3m7UkX1M0/ahYMdfMvbDVLOHAWLlwosVis9VRdXS2pYuPGjVJSUiKbNm1qd/nx48elvLxcpk2blpYbp79RN/+oWTDUzb8w1izhwMnLy5P8/Px2p1Rx9OhRqa2tlQkTJsjmzZvdZSdOnJApU6bIhg0b3PBfum0YC9TNP2oWDHXzL5Q18wKKxWKe3r2wsNBLBevXr/dyc3O9nJwc1674+fjx472GhgYvFQwbNsy1Sc9TBXXzj5oFQ93CWTPNAG2TZkJvQhM4qqqqqnWD6Km0tNQ7duyYlypSbWeOo27+UbNgqFv4auYncEK1LLqsrExWr14tOTk5MnbsWKmqqpKBAwcmu1kpj7r5R82CoW6ZXbOEl0Wni0mTJsmhQ4dk8ODBkp0dupfXb6ibf9QsGOqWuTVL35b3QJdtwz/q5h81C4a6ZWbNQjWkBgBIXQQOAMAEgQMAMEHgAABMEDgAABMEDgDABIEDADBB4AAATBA4AAATBA4AwASBAwAwQeAAAEwQOAAAEwQOAMAEgQMAMEHgAABMEDgAABMEDgDABIEDADBB4AAATBA4AAATBA4AwASBAwAwQeAAAEwQOAAAEwQOAMAEgQMAMEHgAABMEDgAABMRz/O8IHesq6uTaDQqkUhEioqK+r5lIVRTUyMtLS2SlZUlhYWFyW5O2qBu/lGzYKibfwcOHBCNkVgsJvn5+X0bOJWVle7U3Nwsu3fvDtA8AEDY9EvgxNHD8Y+jp2Com3/ULBjq1r89nGw5SQUFBbJv376TfZiMMHz4cNm/f7/bkalZ4qibf9QsGOrmn3Y4NKgTwaIBAIAJAgcAYILAAQCYIHAAACYIHACACQIHAGCCwAEAmCBwAAAmCBwAgAkCBwBggsABAJggcAAAJggcAIAJAgcAYILAAQCYIHAAACYIHACACQIHAGCCwAEAmCBwAAAmCBwAgAkCBwBggsABAJggcAAAJggcAIAJAgcAYILAAYAU53merFmzRl5++WVJZwQOAKR42MyePVuuvfZaufTSS+Wll16SdEXgAEAKmzNnjqxYscL9/5tvvilXXHGF7Ny5U9JRRgTO1q1bk92EtETd/KNmwVC37sNm+fLlMnLkSPf3iBEjWkNn165dkm5CHzi6scaMGSOLFi1KdlPSCnXzj5oFQ926duedd0plZaVccMEFsnbtWndZaWmpu6ympkY++clPSn19vaSTbAm5CRMmuKOC+fPny4ABA+SWW25JdpPSAnXzj5oFQ926duONN8rzzz8vd999t9TV1bVePmvWLGlpaZEhQ4bIoEGDJK14AcViMU/vXlhY6KW61157zRsxYoRr79KlS5PWjmHDhrk26Hk6oG7+UbNgqFvP9uzZ49p10003ealGM0DbppnQm7Tt4UydOlV27NiR8O0bGhrc+dy5c6WgoEDKy8slE1E3/6hZMNQNHaVt4OzduzfwpNnhw4clU1E3/6hZMNQNgRcNNDY2unHEtqdk2rJli1ufnsiptrZWRo8e3Xr0NHPmTMlU1M0/ahYMdUPgwKmoqJBoNNp6Ki4ulnQQi8Vk3Lhxsn37djfZtmTJkmQ3KS1QN/+oWTDULXMkHDgLFy50O0b8VF1dLemy5HLbtm0yY8YMt5wQiaFu/lGzYKhb5kh4DicvL8+d0s2CBQukqKhIrr/+eolEIsluTtqgbv5Rs2CoW+ZI20UDicrKypLp06cnuxlph7r5R82CoW6ZI/S/NAAASA0EDgDAROiH1AAg3Z199tlu+Xi6o4cDADBB4AAATBA4AAATBA4AwASBAwAwQeAAAEwQOAAAEwQOAMAEgQMAMEHgAABMEDgAABMEDgDABIEDADBB4AAATBA4AAATBA4AwASBAwAwQeAAAEwQOAAAEwQOAMAEgQMAMEHgAABMEDgAABMEDgDABIEDADBB4AAATBA4AAATBA4AwASBAwAwQeAAAExEPM/zgtwxFovJ0KFD3f8XFhb2dbtC6Y033hAtdyQSkYKCgmQ3J21QN/+oWTDUzb+amhp3fuTIEYlGo30bOJWVle7U1NQkr776aoDmAQDCprq6WoYPH94/PZyWlhY599xz5ZlnnnFHA6mkrq5OiouLXQHy8/MllVx00UWybds2STWpXDNF3fyjZsFQN380Qi688ELZvXu3ZGX1PEuTLQHpA+fm5vbahUom3SiptGHUgAEDUq5NqV4zRd38o2bBUDf/NAt6C5uTXjQwe/bsk7l7RqJmwVA3/6hZMNSt/2oWeEgtlWnXU3teurAh1Y4EUhU1C4a6+UfNMrduoVwWnZeXJ7fffrs7R2KoWTDUzT9qlrl1C2UPBwCQekLZwwEApB4CBwBggsABAJggcAAAJggcAIAJAgcAYILAAQCYIHAAAGLh/wAUaXO1QRMLigAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_policy(policy):\n",
    "    # Define the action symbols: up=↑, down=↓, left=←, right=→\n",
    "    action_symbols = ['↑', '↓', '←', '→']\n",
    "    \n",
    "    # Create a grid for the policy visualization\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    \n",
    "    # Loop through each position in the grid and add the action symbol\n",
    "    for row in range(env.grid_size[0]):\n",
    "        for col in range(env.grid_size[1]):\n",
    "            action = policy[row, col]\n",
    "            ax.text(col, row, action_symbols[action], ha='center', va='center', fontsize=15, color='black')\n",
    "    \n",
    "    # Set up the gridlines\n",
    "    ax.set_xticks(np.arange(-0.5, env.grid_size[1], 1), minor=True)\n",
    "    ax.set_yticks(np.arange(-0.5, env.grid_size[0], 1), minor=True)\n",
    "    ax.grid(which='minor', color='k', linestyle='-', linewidth=2)\n",
    "    \n",
    "    # Set the axis limits\n",
    "    ax.set_xlim(-0.5, env.grid_size[1] - 0.5)\n",
    "    ax.set_ylim(env.grid_size[0] - 0.5, -0.5)\n",
    "    \n",
    "    # Remove axis labels and ticks\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    \n",
    "    # Display the grid with actions\n",
    "    plt.title(\"Optimal Policy Visualization\")\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the learned optimal policy\n",
    "visualize_policy(optimal_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the policy - 1 Mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(policy, num_episodes=100):\n",
    "    total_rewards = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()  # Reset the environment at the start of each episode\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        # Run the agent in the environment following the learned policy\n",
    "        while not done:\n",
    "            row, col, _ = state  # Extract the position (row, col) and treasure information\n",
    "            action = policy[row, col]  # Select the action according to the policy\n",
    "            \n",
    "            # Take the action and observe the outcome\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Update total reward\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Update state\n",
    "            state = next_state\n",
    "        \n",
    "        # Append the total reward for the current episode\n",
    "        total_rewards.append(total_reward)\n",
    "    \n",
    "    # Compute and print the average reward across all episodes\n",
    "    average_reward = np.mean(total_rewards)\n",
    "    print(f\"Average Reward over {num_episodes} episodes: {average_reward:.2f}\")\n",
    "    return average_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZwAAAGwCAYAAACdL9N0AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAIXtJREFUeJzt3QuUlHX9x/HvLHtBkB08mu4uLMiWdjlpmWhYqSUtRMkKuQpGSWpECJSc6BBpWZ5qS6MQWJC0I15KgyQWuhy7SSYIgUZqCngBzwKrEC6zxrK7sPv8z/d3/rNn7zvPw+53Zp55v84Zx53rb77PM/N5fpcZIp7neQIAQD/L6u8nAABAETgAABMEDgDABIEDADBB4AAATBA4AAATBA4AwASBAwAwQeAAAEwQOBlo1apVEolEZO/evRnz3GeffbZ88YtfbP1748aNrh16noqSuY16a8fHP/5xd7KWrOdF3yFwUsB//vMf+fznPy/Dhg2TvLw8KSoqkmnTprnLT8YPf/hDWbdunaSj7373u+6DLn4aNGiQvO9975PbbrtN6urqJN2UlZW51/D22293exvd5rm5uXL48GHJVC+++KLb9skOWvQT/S01JM9jjz3m5ebmegUFBd6tt97q3Xfffd5tt93mFRYWusvXrl0b+LEHDx7sTZ8+vdPlJ06c8I4dO+a1tLR41u6//3797T5vz549Pd7u9ttvd7dbsWKF99BDD7nzyZMnu8suueQS320fOXJku1o0Nze7Gui5hUcffdS1/YEHHujy+qNHj7rtNXHixKRvo962V2Njozv1hzVr1rjne+KJJzpd15/PCxvZ/RVk6N2rr74qX/jCF6SkpESefPJJecc73tF63de+9jW59NJL3fXPPfecu01fGTBggDulg/LycjnjjDPc/3/lK1+Rq6++WtauXStbtmyRSy65JPDjZmVlycCBA8WyhzNkyBD51a9+Jddff32n66uqquTo0aOul5Pq20h7YZn0vOg7DKkl0V133SX19fXy85//vF3YKP2QXblypfsQuvPOOzsNNe3cuVOuvfZayc/Pl9NPP90FVENDQ+vt9DZ63wceeKB1WCo+h9HVuLzOcVx55ZVuTmP06NFyyimnyHnnndc6x6Ef8vq3fkhfeOGF8q9//atdezUU9fE1GPU2BQUFcuONN/b58NAVV1zhzvfs2ePO9TV+/etfl+LiYjcc+e53v1t+8pOfaM+9x8fpbg5n69at8ulPf1pOO+00GTx4sJx//vly9913u+vuv/9+d5+Orz0+fKkBsX///i6fT+v52c9+Vv7617/KwYMHO12vQaSBpMHU3Tbavn27jB8/3u0b+nijRo1yNe7tNelj6OX6mH2xvTrOpei+03b4s+0p3pbXX39dbr75Zrd9tO26z15zzTXtXp+2Ty9Tn/jEJzo9RldzOFrLm266Sc466yz3Oj7wgQ+4fb6r16/7hb7X3vnOd7p95aKLLpJt27b1+nrRd+jhJNGGDRvcm1V7Ml257LLL3PW///3vO12nYaPXVVRUuKP9JUuWSG1trTz44IPu+oceeki+9KUvycUXXyxf/vKX3WX6RuvJK6+8Ip/73Odk5syZbk5J36ATJ06Ue+65R771rW+5Dwylz6nPv2vXLtdTUH/+85/ltddekxtuuMF9eOn8k7659Vzbp2/4vuoVKv3A0lDRD+gnnnjCfeh88IMflMcff1y+8Y1vuA/+n/3sZ74eW1+Dhm5hYaELcH0dL730kvzud79zf2tva/bs2fLLX/5SLrjggnb31cv0w1Dn4bqjvRf9MFy9erXMmTOn9fK33nrLtfu6665zH8Zd0Q/WcePGuQOTb37zmzJ06FD3QaoHAkH05fZavHix/O9//2t3mdZ+x44dbjsp/WDfvHmzTJ06VYYPH+7avmLFClcznbfR+S3d37/61a+6fVn3t/e+973uvvHzjo4dO+bur/ut1lMDeM2aNS5Ijxw54rZZx1DXOTTdv/X16YGcHgRoHXJycgJUEb4ZDd2hgyNHjrix6quuuqrH25WVlbnb1dXVtZvb0Mvbuvnmm93l//73v3udw+lqXF7nOPSyzZs3t172+OOPu8tOOeUU7/XXX2+9fOXKlZ3G2evr6zs9zyOPPOJu9+STT/b43F2Jv85du3Z5hw4dcrfX583Ly/POOussN+exbt06d5vvf//77e5bXl7uRSIR75VXXmn3+trWQtve9jXonMmoUaPc7Wpra9s9Xtt5lOuuu84rKipqN/fz7LPPusfS19YTfQ6dm9M5qLbuueced3+td3d1+u1vf+v+3rZtW7eP3/E1xeljdGzfyWyvyy+/3J26s3r1anefO+64o8fne/rpp93tHnzwwYTmcDo+7+LFi91tH3744dbLmpqaXH1PPfXU1vdM/PWffvrp3ltvvdV626qqKnf5hg0bun0t6FsMqSVJfLWSDqP0JH59x5VZeqTd1ty5c935H/7wh8Bt0lVgbedFPvzhD7cOY40YMaLT5XpkGNf2yFyH9v773//KmDFj3N/PPvts4DbpEIwe1evRqx6Zvutd73I9Pj0i1teqw1h6VNyWDrFp7+ePf/xjws+jw2Q6THfLLbe43kNbbY/2df7lwIEDrlfVtnejr1/nl3qibdUj/KeffrrdUJIeeeuQ0NixY7u9b7xN2ts6fvy4nKz+2l7aW9GhuauuusqtKOzq+bT9OnSn21JfV9Dn0+2vvTPtGcZpT0X3B+1x/f3vf293+ylTprih0rj4yELb/Rj9i8BJkniQ9LRMtqdgOuecc9r9rcNlOrx1MstJ24aKikaj7lznR7q6XIfw2g4L6RCGfnDqh0s8JFQsFgvcpscee8wN/+g4vg6dvPDCC24OKT4voEvIO9YmPgSj1/sdqnv/+9/f4+1KS0vdkJuGjGppaZFHHnnEfcD2dvCg4osCNGTUvn375B//+IcLop4WCVx++eUu0L73ve+5ORx9Pp1TamxslCD6Y3vpQZEOUemwog7ttg1qHf76zne+0zrXpq9Bn1OHvoI+n25ffR/Eh3V72/4d9+94+LTdj9G/mMNJEv3Q1g8unbztiV6vb2BdHNCTvpgj6e4Dr7vL207M65yOjtHr/InOpZx66qnuw/hTn/qUOw9Kx/Xjq9RSgdZC57nuvfdeWb58uWzatMn1eHTOKxEalu95z3tcSOk8hZ5rHeNB1NP2/c1vfuPmV3TuT+d8tCexaNEid5nWu7t9oLm5udNl/bG9dO5Ea/HPf/6z0/6qPXANSO1Bai9a939trwbtyewffiSyH6N/0cNJIp2g1mGcp556qsvr9chXeyx6u45efvnldn/r0b++cXUhQVxfTdT3Ro8QdfWVTmbrEfjkyZNdT6Avl3J3ZeTIke4DrmMvUVfwxa9PVHxBhfageqPDano0rx/82tPRI3VdPZYoDRd9Hj2Y0J6OHqXriqlE6LDXD37wA7diTZ9bJ/kfffTRdkfs2mtoq+ORfn9srx/96EfuS8bas9FA7UjDcvr06S4gdfGFPt/HPvaxTm31s8/q9tX3QcfACrL9YYPASSI9utThDJ2b6LgcVYc89HsnOleht+uosrKy3d9Lly515xMmTGi9TJf1dnxD9+eRY8cjRV291J90+bIevS9btqzTCin94Gpbi9586EMfckNK2uaONev4unSptJ7uu+8+N+SnR+nZ2YkPFsR7MzrEpCu5euvdxEOiYzu0Z6Liw2r6AavbQr/T1Zb2xPpze/3lL39x8zW33nqrTJo0qcvb6HN2fD7dZzv2vnSfVYnst7r933jjDfn1r3/detmJEyfc42qPTYchkVoYUksiPbLVZbL6gaPfcdGlvfqhp72aX/ziF24iV4dculrOrD0jXRKsQyA6Cf3www+7oR79HkLb4Rv9MPjpT3/q5jr0seMT/n1Jh0906EuXmeqEsA4B/ulPf2r9rkx/0SXb+n0N/aDTmulr1+fVL1Hq0E1vy8Db0nkAXaarj6kf5LpcWIc89WhZexE6hNWxlzN//nz3/4kOp8XpdvjIRz7i2qkSCRzdTzQ4tDeir0t7dTqsp7XXD16lw1T6PRb9wNXA1dvpIoOO3/vp6+2lk/bay9P9WffDtrQno/NE2kvXpfraRl2covus7pvxZdNxWnsNpx//+Mdubkfne3TRyplnntnpeXW5v35XTYfynnnmGde7156UDnNqeCYypwZjfbzqDQE899xzbrmtLpnNyclxP3Ojfz///PPdLhd+8cUX3fLfIUOGeKeddpo3Z84c91Mobe3cudO77LLL3LJmvU98WXB3y6I/85nPdHo+vd3s2bPbXRZfZnrXXXe1XrZv3z730zNDhw71otGod80113gHDhxwt9M2B10WrUuie/L222978+bNc0uVtXbnnHOOa1fHn4TpbVl03FNPPeWVlpa6uuqy8vPPP99bunRpp+etqanxBgwY4J177rleEJWVle75L7744i6v71gnXXqt+8SIESPc0vAzzzzTu/LKK73t27e3u5/W6+qrr/YGDRrk9ouZM2d6L7zwQqdl0SezvTouT9bruzvF66tLzW+44QbvjDPOcEuWx48f7/bPjttF3XvvvV5JSYmrb9vH6Go59ptvvtn6uPpTUOedd16n5eld7a9t29729aJ/RfQ/1iGH4PSXBnTc/dChQyk1mZ5ptPepPSAdFvv2t7+d7OYAaYE5HCAA/RkWnX/Q37oDkBjmcAAf/va3v7kvN+pKMZ0gb7sqEEDPCBzAhzvuuMN9f+WjH/1o68pAAIlhDgcAYII5HACACQIHAJDaczj6cxL6syL65Sqrn1ABAKQWnZXRLyLrl8s7/pDqSQeO/qSKnpqamlp/YRcAkNmqq6vdP67XL4sG9Gcn4v9Gh34BDr3T333ScmuPUP8dDySGuvlHzYKhbv7V1NS0/v5d/J8u6fMhtfgwmoaNDq2hd5r++k8fa9dT/x0UJIa6+UfNgqFu/mmtNHQSmVph0QAAwASBAwAwQeAAAEwQOAAAEwQOAMAEgQMAMEHgAABMEDgAABMEDgDABIEDADBB4AAATBA4AAATBA4AwASBAwAwQeAAAEwQOAAAEwQOAMAEgQMAMEHgAABMEDgAABMEDgDABIEDADBB4AAATBA4AAATBA4AwASBAwAwEbrAqa+vl3Xr1smOHTuS3RSEHPsarNSHZF8LXeAcPHhQJk+eLMuWLUt2UxBy7GuwcjAk+1roAgcAkJoIHHRr69atyW4CMgT7WmYgcNCl5cuXy5gxY2TRokXJbgpCjn0tcxA46NKECRNkxIgRMn/+fFm8eHGym4MQY1/LHAQOujRq1CjZuHGj+yCYN29e2k9WInWxr2WO7GQ3ALamTp3qa2llQ0ODO587d64UFBRIeXl5P7YOYcK+ho4InAyzd+9e2bVrV6D7Hj58uM/bg/BiX0PgIbXGxkapq6trd0L62bJli3iel9CptrZWRo8e3XrUOXPmzGQ3H2mEfQ2BA6eiokKi0Wjrqbi4WFLtm7hdaWpqkubmZvP2pLtYLCbjxo2T7du3y6xZs2TJkiXJblLKYF/rW+xrmbOvJRw4CxcudDtG/FRdXS2pQiccS0pKZNOmTe0uP378uBsHnjZtWlpunGQvVd22bZvMmDFDKisrk92clMG+1vfY1zJnX0s4cPLy8iQ/P7/dKVUcPXrUdcl1eeXmzZvdZSdOnJApU6bIhg0b3PBfum2YZFuwYIGsWrVKVq5cKZFIJNnNSRnsa32PfS2D9jUvoFgs5undCwsLvVSwfv16Lzc318vJyXHtip+PHz/ea2ho8FLBsGHDXJv0HOlbN/a18Eq1uq1Pg31NM0DbpJnQm9B8D2fixImyZs2adt3O0tJS9wur2jsD+gr7GqxMDNm+FprAUWVlZbJ69WrJycmRsWPHSlVVlQwcODDZzUIIsa/BSlmI9rXQfQ9n0qRJcujQIRk8eLBkZ4fu5SGFsK/ByqSQ7Gvp2/Ie6LJtwAL7GqxEQ7CvhWpIDQCQuggcAIAJAgcAYILAAQCYIHAAACYIHACACQIHAGCCwAEAmCBwAAAmCBwAgAkCBwBggsABAJggcAAAJggcAIAJAgcAYILAAQCYIHAAACYIHACACQIHAGCCwAEAmCBwAAAmCBwAgAkCBwBggsABAJggcAAAJggcAIAJAgcAYILAAQCYIHAAACYinud5Qe5YV1cn0WhUIpGIFBUV9X3LQqimpkZaWlokKytLCgsLk92ctEHd/KNmwVA3/w4cOCAaI7FYTPLz8/s2cCorK92publZdu/eHaB5AICw6ZfAiaOH4x9HT8FQN/+oWTDUrX97ONlykgoKCmTfvn0n+zAZYfjw4bJ//363I1OzxFE3/6hZMNTNP+1waFAngkUDAAATBA4AwASBAwAwQeAAAEwQOAAAEwQOAMAEgQMAMEHgAABMEDgAABMEDgDABIEDADBB4AAATBA4AAATBA4AwASBAwAwQeAAAEwQOAAAEwQOAMAEgQMAMEHgAABMEDgAABMEDgDABIEDADBB4AAATBA4AAATBA4AwEToAqe+vl7WrVsnO3bsSHZT0gp184+aBUPdMrdmoQucgwcPyuTJk2XZsmXJbkpaoW7+UbNgqFvm1ix0gQMASE0EDgDABIEDADBB4AAATBA4AAATBA4AwASBAwAwkZ3oDRsbG90prq6urr/aBADI5B5ORUWFRKPR1lNxcbGk2jdxu9LU1CTNzc3m7UkX1M0/ahYMdfMvbDVLOHAWLlwosVis9VRdXS2pYuPGjVJSUiKbNm1qd/nx48elvLxcpk2blpYbp79RN/+oWTDUzb8w1izhwMnLy5P8/Px2p1Rx9OhRqa2tlQkTJsjmzZvdZSdOnJApU6bIhg0b3PBfum0YC9TNP2oWDHXzL5Q18wKKxWKe3r2wsNBLBevXr/dyc3O9nJwc1674+fjx472GhgYvFQwbNsy1Sc9TBXXzj5oFQ93CWTPNAG2TZkJvQhM4qqqqqnWD6Km0tNQ7duyYlypSbWeOo27+UbNgqFv4auYncEK1LLqsrExWr14tOTk5MnbsWKmqqpKBAwcmu1kpj7r5R82CoW6ZXbOEl0Wni0mTJsmhQ4dk8ODBkp0dupfXb6ibf9QsGOqWuTVL35b3QJdtwz/q5h81C4a6ZWbNQjWkBgBIXQQOAMAEgQMAMEHgAABMEDgAABMEDgDABIEDADBB4AAATBA4AAATBA4AwASBAwAwQeAAAEwQOAAAEwQOAMAEgQMAMEHgAABMEDgAABMEDgDABIEDADBB4AAATBA4AAATBA4AwASBAwAwQeAAAEwQOAAAEwQOAMAEgQMAMEHgAABMEDgAABMRz/O8IHesq6uTaDQqkUhEioqK+r5lIVRTUyMtLS2SlZUlhYWFyW5O2qBu/lGzYKibfwcOHBCNkVgsJvn5+X0bOJWVle7U3Nwsu3fvDtA8AEDY9EvgxNHD8Y+jp2Com3/ULBjq1r89nGw5SQUFBbJv376TfZiMMHz4cNm/f7/bkalZ4qibf9QsGOrmn3Y4NKgTwaIBAIAJAgcAYILAAQCYIHAAACYIHACACQIHAGCCwAEAmCBwAAAmCBwAgAkCBwBggsABAJggcAAAJggcAIAJAgcAYILAAQCYIHAAACYIHACACQIHAGCCwAEAmCBwAAAmCBwAgAkCBwBggsABAJggcAAAJggcAIAJAgcAYCJ0gVNfXy/r1q2THTt2JLspALrAezRzaxa6wDl48KBMnjxZli1bluymAOgC79HMrVnoAgcAkJoIHHRr69atyW5C2qFmQPcIHHRp+fLlMmbMGFm0aFGym5I2qBnQMwIHXZowYYKMGDFC5s+fL4sXL052c9ICNQN6RuCgS6NGjZKNGze6D9B58+al/WSlBWoG9Cy7l+sRMlOnTvW1tLKhocGdz507VwoKCqS8vFwyDTUD+gaBk2H27t0ru3btCnTfw4cPSyaiZoDxkFpjY6PU1dW1OyH9bNmyRTzPS+hUW1sro0ePbj1anzlzpmQiagYYB05FRYVEo9HWU3FxsaTaN3G70tTUJM3NzebtSXexWEzGjRsn27dvl1mzZsmSJUuS3aSUR816xnvUv7DVLOHAWbhwoXtDxU/V1dWSKnSitqSkRDZt2tTu8uPHj7vx82nTpqXlxkn2Et9t27bJjBkzpLKyMtnNSQvUrHu8R/0LY80SDpy8vDzJz89vd0oVR48edUMZuix18+bN7rITJ07IlClTZMOGDW74L902TLItWLBAVq1aJStXrpRIJJLs5qQFatY93qP+hbJmXkCxWMzTuxcWFnqpYP369V5ubq6Xk5Pj2hU/Hz9+vNfQ0OClgmHDhrk26TkSR93CUTPeo+GsmWaAtkkzoTeh+R7OxIkTZc2aNe26naWlpe4XVrV3BiC5eI/6F7aahSZwVFlZmaxevVpycnJk7NixUlVVJQMHDkx2swD8P96jmV2z0H0PZ9KkSXLo0CEZPHiwZGeH7uUBaY/3aObWLH1b3gNdtg0gdfEezcyahWpIDQCQuggcAIAJAgcAYILAAQCYIHAAACYIHACACQIHAGCCwAEAmCBwAAAmCBwAgAkCBwBggsABAJggcAAAJggcAIAJAgcAYILAAQCYIHAAACYIHACACQIHAGCCwAEAmCBwAAAmCBwAgAkCBwBggsABAJggcAAAJggcAIAJAgcAYILAAQCYIHAAACYinud5Qe5YV1cn0WhUIpGIFBUV9X3LQqimpkZaWlokKytLCgsLk92ctEHd/KNmwVA3/w4cOCAaI7FYTPLz8/s2cCorK92publZdu/eHaB5AICw6ZfAiaOH4x9HT8FQN/+oWTDUrX97ONlykgoKCmTfvn0n+zAZYfjw4bJ//363I1OzxFE3/6hZMNTNP+1waFAngkUDAAATBA4AwASBAwAwQeAAAEwQOAAAEwQOAMAEgQMAMEHgAABMEDgAABMEDgDABIEDADBB4AAATBA4AAATBA4AwASBAwAwQeAAAEwQOAAAEwQOAMAEgQMAMEHgAABMEDgAABMEDgDABIEDADBB4AAATBA4AAATBA4AwEToAqe+vl7WrVsnO3bsSHZT0gp184+aBUPdMrdmoQucgwcPyuTJk2XZsmXJbkpaoW7+UbNgqFvm1ix0gQMASE0EDgDABIEDADBB4AAATBA4AAATBA4AwASBAwAwkZ3oDRsbG90prq6urr/aBADI5B5ORUWFRKPR1lNxcbGk2jdxu9LU1CTNzc3m7UkX1M0/ahYMdfMvbDVLOHAWLlwosVis9VRdXS2pYuPGjVJSUiKbNm1qd/nx48elvLxcpk2blpYbp79RN/+oWTDUzb8w1izhwMnLy5P8/Px2p1Rx9OhRqa2tlQkTJsjmzZvdZSdOnJApU6bIhg0b3PBfum0YC9TNP2oWDHXzL5Q18wKKxWKe3r2wsNBLBevXr/dyc3O9nJwc1674+fjx472GhgYvFQwbNsy1Sc9TBXXzj5oFQ93CWTPNAG2TZkJvQhM4qqqqqnWD6Km0tNQ7duyYlypSbWeOo27+UbNgqFv4auYncEK1LLqsrExWr14tOTk5MnbsWKmqqpKBAwcmu1kpj7r5R82CoW6ZXbOEl0Wni0mTJsmhQ4dk8ODBkp0dupfXb6ibf9QsGOqWuTVL35b3QJdtwz/q5h81C4a6ZWbNQjWkBgBIXQQOAMAEgQMAMEHgAABMEDgAABMEDgDABIEDADBB4AAATBA4AAATBA4AwASBAwAwQeAAAEwQOAAAEwQOAMAEgQMAMEHgAABMEDgAABMEDgDABIEDADBB4AAATBA4AAATBA4AwASBAwAwQeAAAEwQOAAAEwQOAMAEgQMAMEHgAABMEDgAABMRz/O8IHesq6uTaDQqkUhEioqK+r5lIVRTUyMtLS2SlZUlhYWFyW5O2qBu/lGzYKibfwcOHBCNkVgsJvn5+X0bOJWVle7U3Nwsu3fvDtA8AEDY9EvgxNHD8Y+jp2Com3/ULBjq1r89nGw5SQUFBbJv376TfZiMMHz4cNm/f7/bkalZ4qibf9QsGOrmn3Y4NKgTwaIBAIAJAgcAYILAAQCYIHAAACYIHACACQIHAGCCwAEAmCBwAAAmCBwAgAkCBwBggsABAJggcAAAJggcAIAJAgcAYILAAQCYIHAAACYIHACACQIHAGCCwAEAmCBwAAAmCBwAgAkCBwBggsABAJggcAAAJggcAIAJAgcAYILAAYAU53merFmzRl5++WVJZwQOAKR42MyePVuuvfZaufTSS+Wll16SdEXgAEAKmzNnjqxYscL9/5tvvilXXHGF7Ny5U9JRRgTO1q1bk92EtETd/KNmwVC37sNm+fLlMnLkSPf3iBEjWkNn165dkm5CHzi6scaMGSOLFi1KdlPSCnXzj5oFQ926duedd0plZaVccMEFsnbtWndZaWmpu6ympkY++clPSn19vaSTbAm5CRMmuKOC+fPny4ABA+SWW25JdpPSAnXzj5oFQ926duONN8rzzz8vd999t9TV1bVePmvWLGlpaZEhQ4bIoEGDJK14AcViMU/vXlhY6KW61157zRsxYoRr79KlS5PWjmHDhrk26Hk6oG7+UbNgqFvP9uzZ49p10003ealGM0DbppnQm7Tt4UydOlV27NiR8O0bGhrc+dy5c6WgoEDKy8slE1E3/6hZMNQNHaVt4OzduzfwpNnhw4clU1E3/6hZMNQNgRcNNDY2unHEtqdk2rJli1ufnsiptrZWRo8e3Xr0NHPmTMlU1M0/ahYMdUPgwKmoqJBoNNp6Ki4ulnQQi8Vk3Lhxsn37djfZtmTJkmQ3KS1QN/+oWTDULXMkHDgLFy50O0b8VF1dLemy5HLbtm0yY8YMt5wQiaFu/lGzYKhb5kh4DicvL8+d0s2CBQukqKhIrr/+eolEIsluTtqgbv5Rs2CoW+ZI20UDicrKypLp06cnuxlph7r5R82CoW6ZI/S/NAAASA0EDgDAROiH1AAg3Z199tlu+Xi6o4cDADBB4AAATBA4AAATBA4AwASBAwAwQeAAAEwQOAAAEwQOAMAEgQMAMEHgAABMEDgAABMEDgDABIEDADBB4AAATBA4AAATBA4AwASBAwAwQeAAAEwQOAAAEwQOAMAEgQMAMEHgAABMEDgAABMEDgDABIEDADBB4AAATBA4AAATBA4AwASBAwAwQeAAAExEPM/zgtwxFovJ0KFD3f8XFhb2dbtC6Y033hAtdyQSkYKCgmQ3J21QN/+oWTDUzb+amhp3fuTIEYlGo30bOJWVle7U1NQkr776aoDmAQDCprq6WoYPH94/PZyWlhY599xz5ZlnnnFHA6mkrq5OiouLXQHy8/MllVx00UWybds2STWpXDNF3fyjZsFQN380Qi688ELZvXu3ZGX1PEuTLQHpA+fm5vbahUom3SiptGHUgAEDUq5NqV4zRd38o2bBUDf/NAt6C5uTXjQwe/bsk7l7RqJmwVA3/6hZMNSt/2oWeEgtlWnXU3teurAh1Y4EUhU1C4a6+UfNMrduoVwWnZeXJ7fffrs7R2KoWTDUzT9qlrl1C2UPBwCQekLZwwEApB4CBwBggsABAJggcAAAJggcAIAJAgcAYILAAQCYIHAAAGLh/wAUaXO1QRMLigAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward over 100 episodes: 15.00\n",
      "Optimal Value Function V* after Value Iteration:\n",
      "29.22 31.62 35.13 39.53 38.37\n",
      "32.47 35.13 39.03 43.37 42.63\n",
      "36.08 39.03 43.37 42.63 47.37\n",
      "34.53 38.37 42.63 47.37 52.63\n",
      "38.37 42.63 47.37 52.63 47.37\n",
      "Final average reward : 15.00\n"
     ]
    }
   ],
   "source": [
    "# Main execution block\n",
    "\n",
    "# Step 1: Perform value iteration (already done, assuming V is populated)\n",
    "# Note: Ensure the V array has been filled during the value iteration process\n",
    "\n",
    "# Step 2: Improve the policy using the value function from value iteration\n",
    "optimal_policy = policy_improvement()\n",
    "\n",
    "# Step 3: Visualize the learned optimal policy\n",
    "visualize_policy(optimal_policy)\n",
    "\n",
    "# Step 4: Evaluate the optimal policy over multiple episodes\n",
    "average_reward = evaluate_policy(optimal_policy, num_episodes=100)\n",
    "\n",
    "# Step 5: Print the optimal value function for reference\n",
    "print(\"Optimal Value Function V* after Value Iteration:\")\n",
    "for row in range(env.grid_size[0]):\n",
    "    value_row = []\n",
    "    for col in range(env.grid_size[1]):\n",
    "        value_row.append(f\"{V[row, col]:.2f}\")\n",
    "    print(\" \".join(value_row))\n",
    "\n",
    "# Optional: Print or log the final average reward after evaluation\n",
    "print(f\"Final average reward : {average_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**:\n",
    "\n",
    "The reinforcement learning agent successfully learned the optimal policy for navigating the **5x5 FrozenLake** environment using dynamic programming techniques, specifically value iteration. \n",
    "\n",
    "The state-value function V* after value iteration demonstrated significant value increases as the agent approached the goal, indicating effective policy learning. For instance, the value at the goal state (G) reached 52.63, while values near treasure tiles were also high, such as 47.37. The optimal policy derived from the value iteration showed a clear path for the agent, emphasizing moves that maximize rewards while avoiding hazards. The policy included strategic directions such as moving down and right to navigate the grid efficiently.\n",
    "\n",
    "The optimal value function V* values were consistently high near the goal and treasure tiles, reflecting the agent's prioritization of high-reward states. The agent's performance, evaluated over 100 episodes, yielded an average reward of 15.00, showcasing the effectiveness of the learned policy in maximizing rewards.\n",
    "\n",
    "The comparison of the agent's performance with and without treasures highlighted the trade-offs in reward maximization. The inclusion of treasure tiles significantly influenced the agent's navigation strategy, leading to higher cumulative rewards.\n",
    "\n",
    "Visualizing the agent’s direction on the map using the learned policy provided a clear representation of the optimal path, reinforcing the effectiveness of the value iteration approach. The calculated expected total rewards over multiple episodes validated the agent's ability to consistently achieve high performance.\n",
    "\n",
    "In conclusion, the reinforcement learning agent, through dynamic programming and value iteration, effectively learned to navigate the FrozenLake environment, maximizing rewards and demonstrating robust decision-making capabilities in a complex, stochastic setting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
