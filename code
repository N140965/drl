import numpy as np
import gym
from gym import spaces

class TreasureHuntEnv(gym.Env):
    def __init__(self):
        super(TreasureHuntEnv, self).__init__()
        
        # Define grid layout (5x5 grid with treasures)
        self.grid = [
            ['S', 'F', 'F', 'H', 'T'],
            ['F', 'H', 'F', 'F', 'F'],
            ['F', 'F', 'F', 'T', 'F'],
            ['T', 'F', 'H', 'F', 'F'],
            ['F', 'F', 'F', 'F', 'G']
        ]
        
        self.grid_size = (5, 5)  # Grid size 5x5
        self.treasure_positions = [(0, 4), (2, 3), (3, 0)]  # Coordinates of treasure tiles (T)
        self.start = (0, 0)  # Starting position (S)
        self.goal = (4, 4)  # Goal position (G)
        
        # Define action space: 4 actions (up, down, left, right)
        self.action_space = spaces.Discrete(4)
        
        # Define observation space: (row, col, treasure_collected)
        self.observation_space = spaces.Tuple((
            spaces.Discrete(self.grid_size[0]),  # Row
            spaces.Discrete(self.grid_size[1]),  # Column
            spaces.MultiBinary(len(self.treasure_positions))  # Treasures collected
        ))

        # Initialize state (row, col, treasures_collected)
        self.state = (0, 0, [False] * len(self.treasure_positions))  # Start position with no treasures collected

    def reset(self):
        """Reset the environment to the initial state."""
        self.state = (0, 0, [False] * len(self.treasure_positions))  # Reset state to start position
        return self.state

    def step(self, action):
        """Execute one step of the environment, based on the action."""
        row, col, treasures_collected = self.state
        
        # Define movement directions (up, down, left, right)
        move_dict = {
            0: (-1, 0),  # up
            1: (1, 0),   # down
            2: (0, -1),  # left
            3: (0, 1)    # right
        }
        
        # Calculate new position
        new_row = row + move_dict[action][0]
        new_col = col + move_dict[action][1]
        
        # Ensure the new position is within grid bounds
        if new_row < 0 or new_row >= self.grid_size[0] or new_col < 0 or new_col >= self.grid_size[1]:
            new_row, new_col = row, col  # If out of bounds, stay in place
        
        # Get the tile at the new position
        tile = self.grid[new_row][new_col]
        
        # Update treasure collection status
        if tile == 'T' and not treasures_collected[self.treasure_positions.index((new_row, new_col))]:
            treasures_collected[self.treasure_positions.index((new_row, new_col))] = True
            reward = 5  # Collecting a treasure
            tile = 'F'  # Convert the treasure tile into a frozen tile (F)
        elif tile == 'F':
            reward = 0  # No reward for frozen tiles
        elif tile == 'H':
            reward = -10  # Falling into a hole (H)
            done = True
        elif tile == 'G':
            reward = 10  # Reaching the goal (G)
            done = True
        else:
            reward = 0  # Default reward for other tiles
        
        # Update the state with the new position and treasure collection status
        self.state = (new_row, new_col, treasures_collected)
        
        return self.state, reward, done, {}

    def render(self):
        """Visualize the environment."""
        print(f"Position: {self.state[:2]} | Treasures collected: {self.state[2]}")
        for row in self.grid:
            print(" ".join(row))

import numpy as np

# Set up the environment
env = TreasureHuntEnv()

# Parameters for Value Iteration
gamma = 0.9  # Discount factor (importance of future rewards)
theta = 1e-6  # Convergence threshold (stop when the value function is stable)

# Initialize the value function (V) for each state in the grid
V = np.zeros((env.grid_size[0], env.grid_size[1]))  # Initialize state values to 0

# Initialize the value function for treasures collected
V_treasure_collected = np.zeros(len(env.treasure_positions))  # No treasures collected initially

# Define the Bellman Update for Value Iteration
def get_reward(state, action, treasures_collected):
    row, col, _ = state
    move_dict = {
        0: (-1, 0),  # up
        1: (1, 0),   # down
        2: (0, -1),  # left
        3: (0, 1)    # right
    }

    # Calculate new position
    new_row = row + move_dict[action][0]
    new_col = col + move_dict[action][1]

    # Ensure the new position is within grid bounds
    if new_row < 0 or new_row >= env.grid_size[0] or new_col < 0 or new_col >= env.grid_size[1]:
        return state, 0, False  # Invalid move, no reward

    # Get the tile at the new position
    tile = env.grid[new_row][new_col]

    # Update treasure collection status
    if tile == 'T' and not treasures_collected[env.treasure_positions.index((new_row, new_col))]:
        treasures_collected[env.treasure_positions.index((new_row, new_col))] = True
        reward = 5  # Collecting a treasure
        tile = 'F'  # Convert the treasure tile into a frozen tile (F)
    elif tile == 'F':
        reward = 0  # No reward for frozen tiles
    elif tile == 'H':
        reward = -10  # Falling into a hole (H)
        done = True
    elif tile == 'G':
        reward = 10  # Reaching the goal (G)
        done = True
    else:
        reward = 0  # Default reward for other tiles

    return (new_row, new_col, treasures_collected), reward, tile == 'H' or tile == 'G'

# Value Iteration Algorithm
def value_iteration():
    global V
    while True:
        delta = 0  # Track how much the value function changes
        # Iterate over all positions (states) in the grid
        for row in range(env.grid_size[0]):
            for col in range(env.grid_size[1]):
                old_value = V[row, col]
                action_values = []
                # Check all possible actions
                for action in range(env.action_space.n):
                    # Get reward and next state
                    next_state, reward, done = get_reward((row, col, [False] * len(env.treasure_positions)), action, [False] * len(env.treasure_positions))
                    # Calculate the value for each action
                    action_values.append(reward + gamma * V[next_state[0], next_state[1]])
                
                # Bellman update: Take the max value of all possible actions
                V[row, col] = max(action_values)
                delta = max(delta, abs(old_value - V[row, col]))  # Track largest change in value

        # Convergence check: If the change in value is smaller than the threshold, we stop
        if delta < theta:
            break

# Run the value iteration process
value_iteration()

# Display the final state-value function V*
print("State-value function V* after value iteration:")
print(V)
